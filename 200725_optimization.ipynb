{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "colab": {
      "name": "200725-optimization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taji99/hello-world/blob/master/200725_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hnqbfdESS79",
        "colab_type": "text"
      },
      "source": [
        "# 最適化アルゴリズム\n",
        "勾配降下法では、勾配をもとに重みとバイアスを少しずつ調整し、誤差が最小になるようにネットワークを最適化します。  \n",
        "最適化アルゴリズムは、この最適化のための具体的なアルゴリズムです。 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQCZgJgESS7_",
        "colab_type": "text"
      },
      "source": [
        "## ●最適化アルゴリズムの概要\n",
        "最適化アルゴリズムは、例えるなら夜の山岳地帯で谷底を目指すための戦略です。  \n",
        "闇夜なので何も見えず、足元の傾斜のみが頼りです。  \n",
        "\n",
        "その際に、現時点で最も急な方向に降るのがいいのか、それともこれまでの経路を考慮に入れて進行方向を決めるのがいいのか、谷底にたどり着くための様々な戦略を考えることができます。  \n",
        "戦略を誤ると、局所的に凹んだ地形に囚われてしまうかもしれませんし、谷底にたどり着くまで時間がかかりすぎてしまうかもしれません。  \n",
        "\n",
        "効率的に、なおかつ確実に大域最適解にたどり着くために、最適化アルゴリズムの選択は重要です。  \n",
        "これまでに、様々な最適化アルゴリズムが考案されています。\n",
        "今回は、このうち代表的なものをいくつか紹介します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31WUG3rdSS8A",
        "colab_type": "text"
      },
      "source": [
        "## ●確率的勾配降下法（SGD）\n",
        "\n",
        "確率的勾配降下法（Stochastic gradient descent、SGD）は更新毎にランダムにサンプルを選び出すアルゴリズムです。  \n",
        "以下は、確率的勾配降下法の更新式です。\n",
        "\n",
        "$$ w \\leftarrow w-\\eta\\frac{\\partial E}{\\partial w} $$\n",
        "$$ b \\leftarrow b-\\eta\\frac{\\partial E}{\\partial b} $$\n",
        "\n",
        "以前に、勾配降下法の解説に使用したのはこの式です。  \n",
        "確率的勾配降下法は、訓練用のデータの中から更新毎にランダムにサンプルを選び出すため、局所最適解に囚われにくいというメリットがあります。\n",
        "\n",
        "また、学習係数と勾配をかけてシンプルに更新量が決まります。  \n",
        "簡単なコードで実装できるのもメリットの一つです。  \n",
        "欠点は、学習の進行に応じて柔軟に更新量の調整ができない点です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V43mA7B9SS8B",
        "colab_type": "text"
      },
      "source": [
        "## ●Momentum\n",
        "Momentumは、確率的勾配降下法に慣性項を付け加えたアルゴリズムです。  \n",
        "以下は、Momentumの更新式です。  \n",
        "\n",
        "$$ w \\leftarrow w-\\eta\\frac{\\partial E}{\\partial w} + \\alpha\\Delta w $$\n",
        "$$ b \\leftarrow b-\\eta\\frac{\\partial E}{\\partial b} + \\alpha\\Delta w $$\n",
        "\n",
        "これらの式において、$\\alpha$は慣性の強さを決める定数で、$\\Delta w$は前回の更新量です。  \n",
        "$\\alpha\\Delta w$の慣性項により、新たな更新量はこれまでの更新量の影響を受けるようになります。  \n",
        "これにより更新量の急激な変化が防がれ、より滑らかな更新が実現されます。  \n",
        "\n",
        "その一方で、確率的勾配降下法と比較して設定しなければいけない定数が$\\eta$、$\\alpha$と2つに増えるので、調整がより難しくなります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzLEBzW2SS8B",
        "colab_type": "text"
      },
      "source": [
        "## ●AdaGrad\n",
        "AdaGradは更新量が自動的に調整されるのが強みです。  \n",
        "学習が進むと、学習率が次第に小さくなっていきます。  \n",
        "以下は、AdaGradの重みの更新式です。  \n",
        "\n",
        "$$ h \\leftarrow h+ (\\frac{\\partial E}{\\partial w})^2 $$\n",
        "$$ w \\leftarrow w-\\eta\\frac{1}{\\sqrt{h}}\\frac{\\partial E}{\\partial w} $$\n",
        "\n",
        "バイアスの更新式は、重みの更新式と同じになります。  \n",
        "上記の式では$h$が必ず増加するように更新します。  \n",
        "$h$は式の分母にあるので、更新量は必ず減少していくことになります。  \n",
        "$h$は重みごとに計算されるので、これまでの総更新量が少ない重みは新たな更新量が大きくなり、総更新量が多い重みは新たな更新量が小さくなります。  \n",
        "これにより、最初は広い領域で探索し、次第に探索範囲を絞るという効率のいい探索が可能になります。  \n",
        "\n",
        "また、AdaGradには設定すべき定数が$\\eta$しかないので、定数の調整に手間がかからず手軽に導入することができます。  \n",
        "AdaGradの弱点は、更新量が常に減少するため途中で更新量がほぼ0になってしまい、それ以上最適化が進まなくなってしまうことがある点です。  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhX04BUySS8C",
        "colab_type": "text"
      },
      "source": [
        "## ●RMSProp\n",
        "AdaGradの、更新量の低下により学習が停滞する弱点を克服したものが、RMSPropです。  \n",
        "以下は、RMSPropの重みの更新式です。  \n",
        "\n",
        "$$ h \\leftarrow \\rho h+ (1-\\rho)(\\frac{\\partial E}{\\partial w})^2 $$\n",
        "$$ w \\leftarrow w-\\eta\\frac{1}{\\sqrt{h}}\\frac{\\partial E}{\\partial w} $$\n",
        "\n",
        "バイアスの更新式は、重みの更新式と同じになります。  \n",
        "$\\rho$の存在により、過去の$h$を適当な割合で「忘れる」ことで、AdaGradの弱点に対応しています。  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sXHC3bOSS8D",
        "colab_type": "text"
      },
      "source": [
        "## ●Adam\n",
        "Adam(Adaptive moment estimation)は他の様々なアルゴリズムの良い点を併せ持ち、しばしば他の最適化アルゴリズムよりも高い性能を発揮することがあります。    \n",
        "Adamの重みの更新式は、以下の通りです。\n",
        "\n",
        "$$ \\begin{aligned}\n",
        "m_0 & = v_0 = 0\\\\\n",
        "m_t & = \\beta_1 m_{t-1}+(1-\\beta_1)\\frac{\\partial E}{\\partial w} \\\\\n",
        "v_t & = \\beta_2 v_{t-1}+(1-\\beta_2)(\\frac{\\partial E}{\\partial w})^2 \\\\\n",
        "\\hat{m_t} & = \\frac{m_t}{1-\\beta_1^t} \\\\\n",
        "\\hat{v_t} & = \\frac{v_t}{1-\\beta_2^t} \\\\\n",
        "w & \\leftarrow w-\\eta\\frac{\\hat{m_t}}{\\sqrt{\\hat{v_t}}+\\epsilon}\n",
        "\\end{aligned}  $$\n",
        "\n",
        "バイアスの更新式は、重みの更新式と同じになります。     \n",
        "定数だけでも$\\beta_1$、$\\beta_2$、$\\eta$、$\\epsilon$の4つがありますね。  \n",
        "$t$は反復回数になります。  \n",
        "\n",
        "大まかに言ってMomentumとAdaGradを統合したようなものになっています。  \n",
        "定数の数が多いですが、元の論文には推奨パラメータが記載されています。  \n",
        "上記の式は少々複雑ですが、Kerasなどのフレームワークを使えば最適化アルゴリズムにAdamを指定するのみで簡単に実装することができます。"
      ]
    }
  ]
}